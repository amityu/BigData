# -*- coding: utf-8 -*-
"""webscrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-zQxNAHzdNgTqVKCXeFY3w31rsPGJnzh
"""

#from google.colab import drive
#drive.mount('/content/drive')

"""In this notebook I'll download articles from relators magazine. trying to find out the issues that Realtors were interested in. And trying to get insight on the realestate market from the relators view point

"""

import os
RESOURCE_PATH = '../project_resources/'
#os.mkdir('/content/drive/MyDrive/BigDataProject')
#os.chdir('/content/drive/MyDrive/BigDataProject')

"""Webscrapping, downloading all articles"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
from tqdm import tqdm
def webscrap():
    u2 ='https://www.car.org/en/knowledge/pubs/CREM/Archive'
    u = "https://magazine.realtor/article-archive/all"
    s = requests.get(u).content.decode('utf-8')
    s
    m_url = []
    a_url = []
    p_text = []
    html = s
    soup = BeautifulSoup(html, 'html.parser')
    l = soup.findAll(attrs={'class': "field-content"})
    magazines = []
    # get all magazines url
    for tag in l:
      magazines.append(tag.findChild("a")['href'])
      print('magazine ' + magazines[-1])
      m_html = requests.get(magazines[-1]).content.decode('utf-8')
      m_soup = BeautifulSoup(m_html, 'html.parser')
      article_tags = m_soup.findAll(attrs={'class': "layout-slat__header "})
      article = []
      #get all article urls
      for tag in article_tags:
        article.append(tag.findChild("a")['href']) #add article url
        print('article ' + article[-1])
        a_html = requests.get(article[-1]).content.decode('utf-8')
        a_soup = BeautifulSoup(a_html, 'html.parser')
        for paragraph in a_soup.findAll('p'):
          m_url.append(magazines[-1])
          a_url.append(article[-1])
          p_text.append( paragraph.get_text())



    df  = pd.DataFrame(list(zip(m_url, a_url, p_text)),
                   columns =['m_url', 'a_url', 'p_text'])
    df.to_csv(RESOURCE_PATH + 'realator_p.csv')

    df.head()


def webscrap2():
    u = 'https://realtyquarter.com/category/real_estate_news/page/%d/'
    ua = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'
    ua ='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'

    a_url = []
    p_text = []

    for p in tqdm(range(118)):
        s = requests.get(u%p,headers={"User-Agent": ua}).content.decode('utf-8')
        html = s
        soup = BeautifulSoup(html, 'html.parser')
        l = soup.findAll('h3', attrs={'class': "entry-title"})
        # get all magazines url

        for tag in l:
          #print('magazine ' + a_url[-1])
          a_url.append(tag.findChild("a")['href'])

          try:
            a_html = requests.get(a_url[-1],headers={"User-Agent": ua}).content.decode('utf-8')
          except:

                print('error in %d'%p)
                continue


          a_soup = BeautifulSoup(a_html, 'html.parser')
          for paragraph in a_soup.findAll('p'):
               p_text.append( paragraph.get_text())

    df  = pd.DataFrame(list(zip(a_url, p_text)),
                   columns =['a_url', 'p_text'])
    df.to_csv(RESOURCE_PATH + 'quater_p.csv')

    df.head()
#webscrap2()
#!pip install turicreate

#from google.colab import drive
#drive.mount('/content/drive')
#import os
#os.chdir('/content/drive/MyDrive/BigDataProject')

import nltk
import turicreate as tc
from nltk.tokenize import word_tokenize


#nltk.download('punkt')

import pandas as pd
import os
#os.chdir('/content/drive/MyDrive/BigDataProject')

#nltk.download('stopwords')
from nltk.corpus import stopwords
df = pd.read_csv(RESOURCE_PATH + './quater_p.csv')
df['p_text']=df['p_text'].apply(lambda x: x if len(str(x))>20 else 'NaN')
df = df.replace('NaN', 'None')
df = df.dropna()
sf = tc.SFrame(data = df)

print (sf)


import turicreate.aggregate as agg
from collections import Counter
from nltk.stem.porter import *
from functools import lru_cache


def preprocess(sf):
    sf = sf.dropna()
    sf = sf.groupby(['a_url'], {'paragraph_list': agg.CONCAT('p_text')})
    sf['all_text'] = sf['paragraph_list'].apply(lambda l: ' '.join(l))
    return sf

stop_words_set = set(stopwords.words("english"))

def skip_word(w):
    if len(w) < 2 or w in stop_words_set:
        return True
    return False
stemmer = PorterStemmer()

#Using cahcing for faster performence
@lru_cache(maxsize=None)
def word_stemming(w):
    return stemmer.stem(w)

def word_stem_list(txt):
    return [word_stemming(w) for w in re.findall(r"(\w+)", txt) if not skip_word(w) ]
sf =preprocess(sf)
txt = ' '.join(sf['all_text'])
l = word_stem_list(txt)

d = Counter(l)
print('most common words')
print (d.most_common(20))

# relators
# stop_words_set |= {k for k,v in d.items() if v > 2000}
#quarter
stop_words_set |= {k for k,v in d.items() if v > 150}
def skip_word2(w):
    if len(w) <2:
        return True
    if w.isdigit():
        return True
    if word_stemming(w) in stop_words_set :#or stemmer.stem(w) in stop_words_set:
        return True
    return False

def word_stem_string(txt):
    return ' '.join([word_stemming(w) for w in re.findall(r"(\w+)", txt) if not skip_word2(w) ])

s = []
for  row in sf:
    #sf['stemmed'] =sf['all_text'].apply(word_stem_list)
    s.append(word_stem_string(row['all_text']))
sf['stemmed'] = tc.SArray(s)


def skip_word2(w):
    if len(w) <2:
        return True
    if w.isdigit():
        return True
    if w in stop_words_set :#or stemmer.stem(w) in stop_words_set:
        return True
    return False
#docs = tc.text_analytics.count_words(sf['stemmed'])
docs = tc.text_analytics.count_ngrams(sf['stemmed'], n=1)

#docs = tc.text_analytics.tf_idf(sf['stemmed'])

#docs = docs.apply(lambda d: {w:v for w, v in d.items() if not skip_word2(w)})
#docs = tc.text_analytics.count_words(docs)
#docs = tc.text_analytics.count_ngrams(sf['all_text'], n=2)



print('---------Corpus--------')
print('most common words docs ')
#stop_words_set |= {k for k,v in d.items() if v > 1000}
#docs = {w:v for w, v in d.items() if not skip_word2(w)}

topic_model = tc.topic_model.create(docs,num_topics=15)
topic_model.get_topics().print_rows(100)

topic_model = topic_model_by_year(2018)
topic_model.get_topics().print_rows(50)

topic_model = topic_model_by_year(2020)
topic_model.get_topics().print_rows(50)

rows = topic_model.get_topics()

print(type(rows))
high = rows[rows['score']>0.01]
high.print_rows(50)

import spacy
spacy.require_gpu()

from spacy import displacy

nlp = spacy.load('en')

def get_entites_from_text(text):
    entities_dict= {}
    #using spaCy to get entities
    for entity in doc.ents:
        label = entity.label_
        if  label not in entities_dict:
            entities_dict[label] = set()
        entities_dict[label].add(entity.text)        

    return entities_dict

def get_entites_from_text(d):
    doc = nlp(' '.join(d.keys()))
    
    entities_dict= {}
    #using spaCy to get entities
    for entity in doc.ents:
        label = entity.label_
        if  label not in entities_dict:
            entities_dict[label] = set()
        entities_dict[label].add(entity.text)        
    return entities_dict

nlp.max_length = 6000000000

a_list= get_entites_from_text(docs)

a_list

import json

import turicreate as tc
def count_entities():
  sf = tc.SFrame.read_csv('ner_dict.csv')
  
  count_entities_dict = {}
  dict_list = list(sf['NerDict'])
  for d in dict_list:
     d = json.loads(d)
     for entity_type, entity_set in d.items():
        if entity_type not in count_entities_dict:
            count_entities_dict[entity_type] = {}
        for name in entity_set:
            name = name.replace("\n", " ").strip().lower()
            if name not in count_entities_dict[entity_type]:
                count_entities_dict[entity_type][name] = 0
            count_entities_dict[entity_type][name] += 1
      
     

  return count_entities_dict

count_entities_dict = count_entities()


count_entities_dict

# Commented out IPython magic to ensure Python compatibility.

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
# %matplotlib inline

def draw_entities_word_cloud(entity_type, min_times=10):
    stopwords = set(STOPWORDS) | {"doc", "date","memo", "subject", 'state',
                                 }
    stopwords_parts = {"sent", "subject", "original message", "unclassified" }
    wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='black', 
                    stopwords = stopwords, 
                    min_font_size = 10)
    def skip_entity(e):
        if e in stopwords:
            return True
        for p in stopwords_parts:
            if p in e:
                return True
        return False

    # using the subject frquencies
    d = {k:v for k,v in count_entities_dict[entity_type].items() if v > min_times and not skip_entity(k)}
    wordcloud.generate_from_frequencies(frequencies=docs)
    plt.figure(figsize = (20, 20), facecolor = None) 
    plt.imshow(wordcloud)
    plt.savefig('realtor_wc_' + entity_type +'.png')



for ner in count_entities_dict.keys():

 
  
  draw_entities_word_cloud(ner,2)

draw_entities_word_cloud('EVENT', 2)

count_entities_dict['FAC']['metro']

draw_entities_word_cloud('NORP', 2)
